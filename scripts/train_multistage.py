#!/usr/bin/env python3
"""
å¤šé˜¶æ®µè®­ç»ƒè„šæœ¬ - å®Œæ•´ä¿®å¤ç‰ˆ
æ”¯æŒStage 1/2/3çš„è®­ç»ƒå’Œç»­è®­
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import sys
from pathlib import Path
import argparse
import numpy as np

if __name__ == '__main__':
    import multiprocessing
    multiprocessing.set_start_method('spawn', force=True)
    
    REPO_DIR = Path(__file__).parent.parent
    sys.path.insert(0, str(REPO_DIR))
    
    from envs.intersection_env import IntersectionEnvWrapper
    from stable_baselines3 import PPO
    from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
    from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, BaseCallback


    class DebugEvalCallback(BaseCallback):
        """ä¿®å¤ç‰ˆè¯„ä¼°Callback"""
        def __init__(self, eval_env, eval_freq=5000, n_eval_episodes=5, 
                     log_path="../outputs/logs/', verbose=1):
            super().__init__(verbose)
            self.eval_env = eval_env
            self.eval_freq = eval_freq
            self.n_eval_episodes = n_eval_episodes
            self.log_path = Path(log_path)
            self.evaluations_rewards = []
            self.evaluations_timesteps = []
            self.eval_count = 0
            self.last_eval_step = 0
            
            print(f"\n{'='*60}")
            print(f"ğŸ”§ è¯„ä¼°Callbackå·²åˆå§‹åŒ–")
            print(f"   è¯„ä¼°é¢‘ç‡: æ¯{eval_freq}æ­¥")
            print(f"   è¯„ä¼°episodes: {n_eval_episodes}")
            print(f"{'='*60}\n")
        
        def _on_step(self) -> bool:
            if self.num_timesteps % 1000 == 0 and self.num_timesteps != self.last_eval_step:
                print(f"[Callback] æ­¥æ•°: {self.num_timesteps:,}, n_calls: {self.n_calls}")
            
            if self.num_timesteps - self.last_eval_step >= self.eval_freq:
                self.last_eval_step = self.num_timesteps
                self.eval_count += 1
                
                print(f"\n{'ğŸ”µ'*30}")
                print(f"ğŸ¯ ç¬¬{self.eval_count}æ¬¡è¯„ä¼° (æ­¥æ•°: {self.num_timesteps:,})")
                print(f"{'ğŸ”µ'*30}\n")
                
                episode_rewards = []
                
                try:
                    for ep in range(self.n_eval_episodes):
                        print(f"  Episode {ep+1}/{self.n_eval_episodes}...", end=" ")
                        obs = self.eval_env.reset()
                        done = False
                        ep_reward = 0
                        ep_length = 0
                        
                        while not done and ep_length < 600:
                            action, _ = self.model.predict(obs, deterministic=True)
                            obs, reward, done, info = self.eval_env.step(action)
                            ep_reward += reward[0] if isinstance(reward, np.ndarray) else reward
                            ep_length += 1
                        
                        episode_rewards.append(ep_reward)
                        print(f"Reward: {ep_reward:.2f}, Length: {ep_length}")
                    
                    mean_reward = np.mean(episode_rewards)
                    std_reward = np.std(episode_rewards)
                    
                    print(f"\n  ğŸ’¾ å†™å…¥Tensorboard...")
                    self.logger.record("eval/mean_reward", mean_reward)
                    self.logger.record("eval/std_reward", std_reward)
                    
                    self.evaluations_rewards.append(episode_rewards)
                    self.evaluations_timesteps.append(self.num_timesteps)
                    
                    self._save_npz()
                    
                    print(f"\n{'='*60}")
                    print(f"ğŸ“Š è¯„ä¼°ç»“æœ (æ­¥æ•°: {self.num_timesteps:,})")
                    print(f"   å¹³å‡Reward: {mean_reward:.2f} Â± {std_reward:.2f}")
                    print(f"   å·²è¯„ä¼°: {self.eval_count}æ¬¡")
                    print(f"{'='*60}\n")
                    
                except Exception as e:
                    print(f"\nâŒ è¯„ä¼°å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
            
            return True
        
        def _save_npz(self):
            try:
                self.log_path.mkdir(parents=True, exist_ok=True)
                save_path = self.log_path / "evaluations.npz"
                
                np.savez(
                    save_path,
                    timesteps=np.array(self.evaluations_timesteps),
                    results=np.array(self.evaluations_rewards)
                )
                print(f"  âœ… å·²ä¿å­˜: {save_path}")
            except Exception as e:
                print(f"  âŒ ä¿å­˜å¤±è´¥: {e}")
        
        def _on_training_end(self) -> None:
            self._save_npz()
            print(f"\nâœ… è®­ç»ƒç»“æŸï¼Œå…±è¯„ä¼°{self.eval_count}æ¬¡")


    def create_output_dirs():
        dirs = ["../outputs/models', "../outputs/logs/stage1', "../outputs/logs/stage2', 
                "../outputs/logs/stage3', "../outputs/figures']
        for d in dirs:
            Path(d).mkdir(parents=True, exist_ok=True)


    def make_env(difficulty, rank, seed=42):
        def _init():
            env = IntersectionEnvWrapper(difficulty=difficulty)
            env.reset(seed=seed + rank)
            return env
        return _init


    # è§£æå‚æ•°
    parser = argparse.ArgumentParser(description='å¤šé˜¶æ®µè®­ç»ƒ')
    parser.add_argument('--stage', type=int, required=True, choices=[1, 2, 3])
    parser.add_argument('--n-envs', type=int, default=16)
    parser.add_argument('--timesteps', type=int, default=None)
    parser.add_argument('--from-checkpoint', type=str, default=None)
    args = parser.parse_args()
    
    create_output_dirs()
    
    # æ ¹æ®stageè®¾ç½®å‚æ•°
    difficulty_map = {1: 'easy', 2: 'medium', 3: 'hard'}
    prefix_map = {1: 'ppo_stage1', 2: 'ppo_stage2', 3: 'ppo_stage3'}
    default_timesteps = {1: 200000, 2: 400000, 3: 400000}
    
    difficulty = difficulty_map[args.stage]
    prefix = prefix_map[args.stage]
    
    if args.timesteps is None:
        args.timesteps = default_timesteps[args.stage]
    
    print("="*60)
    print(f"ğŸš€ Stage {args.stage} è®­ç»ƒ")
    print(f"éš¾åº¦: {difficulty}")
    print(f"è®­ç»ƒæ­¥æ•°: {args.timesteps:,}")
    print("="*60)
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nğŸ“¦ åˆ›å»ºç¯å¢ƒ (éš¾åº¦: {difficulty})...")
    try:
        env = SubprocVecEnv([make_env(difficulty, i) for i in range(args.n_envs)], start_method='spawn')
        eval_env = DummyVecEnv([lambda d=difficulty: IntersectionEnvWrapper(difficulty=d)])
        print(f"âœ… {args.n_envs}ä¸ªå¹¶è¡Œç¯å¢ƒ")
    except Exception as e:
        print(f"âš ï¸ é™çº§ä¸º8ä¸ªä¸²è¡Œç¯å¢ƒ: {e}")
        env = DummyVecEnv([make_env(difficulty, i) for i in range(8)])
        eval_env = DummyVecEnv([lambda d=difficulty: IntersectionEnvWrapper(difficulty=d)])
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    if args.from_checkpoint:
        # ä»æŒ‡å®šcheckpointåŠ è½½
        print(f"\nğŸ“¥ ä»checkpointåŠ è½½: {args.from_checkpoint}")
        model = PPO.load(args.from_checkpoint, env=env, device='cpu')
        model.tensorboard_log = f"../outputs/logs/stage{args.stage}/"
        reset_timesteps = False
    elif args.stage == 1:
        # Stage 1: åˆ›å»ºæ–°æ¨¡å‹
        print(f"\nğŸ†• åˆ›å»ºæ–°æ¨¡å‹")
        model = PPO(
            "MlpPolicy", env,
            learning_rate=3e-4, n_steps=2048, batch_size=64,
            n_epochs=10, gamma=0.99, gae_lambda=0.95,
            clip_range=0.2, ent_coef=0.01,
            verbose=1, device='cpu',
            tensorboard_log=f"../outputs/logs/stage{args.stage}/"
        )
        reset_timesteps = True
    else:
        # Stage 2/3: ä»å‰ä¸€ä¸ªstageåŠ è½½
        prev_model = f"../outputs/models/ppo_stage{args.stage-1}_final.zip'
        print(f"\nğŸ“¥ ä»{prev_model}åŠ è½½æ¨¡å‹")
        
        if not Path(prev_model).exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°{prev_model}")
            print(f"è¯·å…ˆå®ŒæˆStage {args.stage-1}çš„è®­ç»ƒï¼")
            sys.exit(1)
        
        model = PPO.load(prev_model, env=env, device='cpu')
        model.tensorboard_log = f"../outputs/logs/stage{args.stage}/"
        reset_timesteps = False
    
    # Callbacks
    print(f"\nğŸ”§ è®¾ç½®Callbacks...")
    eval_callback = DebugEvalCallback(
        eval_env,
        eval_freq=5000,
        n_eval_episodes=5,
        log_path=f"../outputs/logs/stage{args.stage}/',
        verbose=1
    )
    
    checkpoint = CheckpointCallback(
        save_freq=10000,
        save_path="../outputs/models/',
        name_prefix=prefix
    )
    
    callbacks = CallbackList([eval_callback, checkpoint])
    
    print(f"\nğŸ’¡ Tensorboard: tensorboard --logdir ./outputs/logs --reload_interval 5")
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ...")
    print(f"{'='*60}\n")
    
    model.learn(
        total_timesteps=args.timesteps,
        callback=callbacks,
        progress_bar=True,
        reset_num_timesteps=reset_timesteps
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_name = f'ppo_stage{args.stage}_final' if args.stage < 3 else 'ppo_final'
    model.save(f"../outputs/models/{final_name}")
    print(f"\nâœ… Stage {args.stage} è®­ç»ƒå®Œæˆï¼")
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: outputs/models/{final_name}.zip")
    
    env.close()
    eval_env.close()
