#!/usr/bin/env python3
"""
åŸºäºSUMOçš„å¤šé˜¶æ®µè®­ç»ƒè„šæœ¬
æ”¯æŒå››é˜¶æ®µè¯¾ç¨‹å­¦ä¹ å’Œå¯é€‰çš„LLMè®­ç»ƒé¡¾é—®

Usage:
    # Stage 1: ç©ºè·¯å¯¼èˆª
    python train_sumo.py --stage 1 --timesteps 500000
    
    # Stage 2: çº¢ç»¿ç¯éµå®ˆ (å¯ç”¨LLMé¡¾é—®)
    python train_sumo.py --stage 2 --timesteps 800000 --llm --llm-api-key YOUR_KEY
    
    # Stage 3: åŠ¨æ€é¿éšœ
    python train_sumo.py --stage 3 --timesteps 1000000 --llm --llm-api-key YOUR_KEY
    
    # Stage 4: ç»¼åˆåœºæ™¯
    python train_sumo.py --stage 4 --timesteps 1500000 --llm --llm-api-key YOUR_KEY
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import sys
import argparse
import numpy as np
from pathlib import Path
from typing import Optional

# æ·»åŠ é¡¹ç›®è·¯å¾„
REPO_DIR = Path(__file__).parent.parent
sys.path.insert(0, str(REPO_DIR))

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList, BaseCallback
from stable_baselines3.common.monitor import Monitor

from envs.sumo_env import make_sumo_env
from utils.llm_advisor import create_llm_advisor


class EpisodeStatCallback(BaseCallback):
    """
    ç»Ÿè®¡Episodeä¿¡æ¯çš„Callback
    ç”¨äºLLMè®­ç»ƒé¡¾é—®å’ŒTensorboardè®°å½•
    """
    
    def __init__(
        self,
        llm_advisor=None,
        eval_freq: int = 5000,
        log_freq: int = 100,
        verbose: int = 1
    ):
        super().__init__(verbose)
        self.llm_advisor = llm_advisor
        self.eval_freq = eval_freq
        self.log_freq = log_freq
        
        # ç»Ÿè®¡
        self.episode_count = 0
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_successes = []
        self.episode_collisions = []
        
        # æœ€è¿‘100ä¸ªepisodeçš„ç»Ÿè®¡
        self.recent_rewards = []
        self.recent_successes = []
        self.recent_collisions = []
    
    def _on_step(self) -> bool:
        # æ£€æŸ¥æ˜¯å¦æœ‰episodeç»“æŸ
        if self.locals.get("dones") is not None:
            dones = self.locals["dones"]
            infos = self.locals.get("infos", [])
            
            for i, done in enumerate(dones):
                if done and i < len(infos):
                    info = infos[i]
                    
                    # æå–episodeä¿¡æ¯
                    episode_reward = info.get("total_reward", 0.0)
                    episode_length = info.get("step", 0)
                    success = float(info.get("goal_reached", False))
                    collision = float(info.get("collision", False))
                    
                    # è®°å½•
                    self.episode_count += 1
                    self.episode_rewards.append(episode_reward)
                    self.episode_lengths.append(episode_length)
                    self.episode_successes.append(success)
                    self.episode_collisions.append(collision)
                    
                    self.recent_rewards.append(episode_reward)
                    self.recent_successes.append(success)
                    self.recent_collisions.append(collision)
                    
                    # ä¿æŒæœ€è¿‘100ä¸ª
                    if len(self.recent_rewards) > 100:
                        self.recent_rewards = self.recent_rewards[-100:]
                        self.recent_successes = self.recent_successes[-100:]
                        self.recent_collisions = self.recent_collisions[-100:]
                    
                    # è®°å½•åˆ°LLMé¡¾é—®
                    if self.llm_advisor:
                        self.llm_advisor.record_episode(info)
                    
                    # å®šæœŸæ‰“å°ç»Ÿè®¡
                    if self.episode_count % self.log_freq == 0 and len(self.recent_rewards) > 0:
                        mean_reward = np.mean(self.recent_rewards)
                        success_rate = np.mean(self.recent_successes) * 100
                        collision_rate = np.mean(self.recent_collisions) * 100
                        
                        print(f"\n{'='*60}")
                        print(f"Episode {self.episode_count} | æ­¥æ•° {self.num_timesteps:,}")
                        print(f"   æœ€è¿‘100ä¸ªepisode:")
                        print(f"   - å¹³å‡Reward: {mean_reward:.2f}")
                        print(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
                        print(f"   - ç¢°æ’ç‡: {collision_rate:.1f}%")
                        print(f"{'='*60}\n")
                    
                    # è®°å½•åˆ°Tensorboard
                    self.logger.record("episode/reward", episode_reward)
                    self.logger.record("episode/length", episode_length)
                    self.logger.record("episode/success", success)
                    self.logger.record("episode/collision", collision)
                    
                    if len(self.recent_rewards) > 0:
                        self.logger.record("episode/mean_reward_100", np.mean(self.recent_rewards))
                        self.logger.record("episode/success_rate_100", np.mean(self.recent_successes))
                        self.logger.record("episode/collision_rate_100", np.mean(self.recent_collisions))
        
        # è°ƒç”¨LLMé¡¾é—®
        if self.llm_advisor and self.episode_count > 0:
            advice = self.llm_advisor.analyze_and_advise(
                current_episode=self.episode_count,
                training_steps=self.num_timesteps
            )
            
            if advice:
                # å¯ä»¥åœ¨è¿™é‡Œæ ¹æ®å»ºè®®è‡ªåŠ¨è°ƒæ•´å‚æ•°ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
                pass
        
        return True


class EvalCallback(BaseCallback):
    """
    è¯„ä¼°Callback
    """
    
    def __init__(
        self,
        eval_env,
        eval_freq: int = 10000,
        n_eval_episodes: int = 10,
        verbose: int = 1
    ):
        super().__init__(verbose)
        self.eval_env = eval_env
        self.eval_freq = eval_freq
        self.n_eval_episodes = n_eval_episodes
        self.last_eval_step = 0
        self.eval_count = 0
    
    def _on_step(self) -> bool:
        if self.num_timesteps - self.last_eval_step >= self.eval_freq:
            self.last_eval_step = self.num_timesteps
            self.eval_count += 1
            
            print(f"\n{'='*30}")
            print(f"è¯„ä¼° #{self.eval_count} (æ­¥æ•°: {self.num_timesteps:,})")
            print(f"{'='*30}\n")
            
            episode_rewards = []
            episode_successes = []
            episode_collisions = []
            episode_red_lights = []
            
            for ep in range(self.n_eval_episodes):
                obs, _ = self.eval_env.reset()
                done = False
                ep_reward = 0
                ep_steps = 0
                while not done:
                    action, _ = self.model.predict(obs, deterministic=True)
                    obs, reward, terminated, truncated, info = self.eval_env.step(action)
                    done = terminated or truncated
                    ep_reward += reward
                    ep_steps += 1
                episode_rewards.append(ep_reward)
                episode_successes.append(float(info.get("goal_reached", False)))
                episode_collisions.append(float(info.get("collision", False)))
                episode_red_lights.append(info.get("red_light_violations", 0))
                print(f"  Episode {ep+1}: Reward={ep_reward:.2f}, Steps={ep_steps}/{info.get('max_steps', '?')}, "
                      f"Success={info.get('goal_reached', False)}, "
                      f"Collision={info.get('collision', False)}, "
                      f"RedLight={info.get('red_light_violations', 0)}/{info.get('route_traffic_lights', '?')}, "
                      f"BgVehicles={info.get('avg_bg_vehicles', 0):.1f}")
            
            mean_reward = np.mean(episode_rewards)
            success_rate = np.mean(episode_successes) * 100
            collision_rate = np.mean(episode_collisions) * 100
            total_red_lights = sum(episode_red_lights)
            episodes_with_violations = sum(1 for r in episode_red_lights if r > 0)
            
            print(f"\nè¯„ä¼°ç»“æœ:")
            print(f"   - å¹³å‡Reward: {mean_reward:.2f}")
            print(f"   - æˆåŠŸç‡: {success_rate:.1f}%")
            print(f"   - ç¢°æ’ç‡: {collision_rate:.1f}%")
            print(f"   - é—¯çº¢ç¯: {total_red_lights}æ¬¡ ({episodes_with_violations}/{self.n_eval_episodes}ä¸ªepisodeè¿è§„)")
            print(f"{'='*60}\n")
            
            # è®°å½•åˆ°Tensorboard
            self.logger.record("eval/mean_reward", mean_reward)
            self.logger.record("eval/success_rate", success_rate)
            self.logger.record("eval/collision_rate", collision_rate)
        
        return True


def make_vec_env(stage: int, map_name: str, n_envs: int, start_method: str = 'spawn'):
    """
    åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ
    
    Args:
        stage: è®­ç»ƒé˜¶æ®µ
        map_name: åœ°å›¾åç§°
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°
        start_method: å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    
    Returns:
        å‘é‡åŒ–ç¯å¢ƒ
    """
    def make_env(rank: int):
        def _init():
            env = make_sumo_env(
                stage=stage,
                map_name=map_name,
                use_gui=False,
                seed=42 + rank
            )
            env = Monitor(env)
            return env
        return _init
    
    # å°è¯•ä½¿ç”¨SubprocVecEnvï¼Œå¦‚æœå¤±è´¥åˆ™é™çº§åˆ°DummyVecEnv
    env = DummyVecEnv([make_env(i) for i in range(n_envs)])
    print(f"åˆ›å»ºäº† {n_envs} ä¸ªç¯å¢ƒ (DummyVecEnv)")
    
    return env


def create_or_load_model(
    env,
    stage: int,
    from_checkpoint: Optional[str] = None,
    device: str = 'cpu'
) -> PPO:
    """
    åˆ›å»ºæˆ–åŠ è½½PPOæ¨¡å‹
    
    Args:
        env: è®­ç»ƒç¯å¢ƒ
        stage: å½“å‰é˜¶æ®µ
        from_checkpoint: checkpointè·¯å¾„ï¼ˆå¯é€‰ï¼‰
        device: è®¾å¤‡
    
    Returns:
        PPOæ¨¡å‹
    """
    tensorboard_log = str(REPO_DIR / "outputs" / "logs" / f"stage{stage}")
    
    if from_checkpoint:
        # ä»æŒ‡å®šcheckpointåŠ è½½
        print(f"ğŸ“¥ ä»checkpointåŠ è½½: {from_checkpoint}")
        model = PPO.load(from_checkpoint, env=env, device=device)
        model.tensorboard_log = tensorboard_log
        reset_timesteps = False
    
    elif stage == 1:
        # Stage 1: åˆ›å»ºæ–°æ¨¡å‹
        print(f"ğŸ†• åˆ›å»ºæ–°çš„PPOæ¨¡å‹")
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=3e-4,  # æ¢å¤åŸå§‹å­¦ä¹ ç‡
            n_steps=2048,
            batch_size=128,
            n_epochs=5,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.01,
            verbose=1,
            device=device,
            tensorboard_log=tensorboard_log
        )
        reset_timesteps = True
    
    else:
        # Stage 2-4: ä»å‰ä¸€é˜¶æ®µåŠ è½½
        prev_model_path = str(REPO_DIR / "outputs" / "models" / f"best_stage{stage-1}" / "ppo_final.zip")
        
        if not Path(prev_model_path).exists():
            print(f"æ‰¾ä¸åˆ°å‰ä¸€é˜¶æ®µæ¨¡å‹: {prev_model_path}")
            print(f"   åˆ›å»ºæ–°æ¨¡å‹...")
            model = PPO(
                "MlpPolicy",
                env,
                learning_rate=3e-4,  # æ¢å¤åŸå§‹å­¦ä¹ ç‡
                n_steps=2048,
                batch_size=128,
                n_epochs=5,
                gamma=0.99,
                gae_lambda=0.95,
                clip_range=0.2,
                ent_coef=0.01,
                verbose=1,
                device=device,
                tensorboard_log=tensorboard_log
            )
            reset_timesteps = True
        else:
            print(f"ğŸ“¥ ä»å‰ä¸€é˜¶æ®µåŠ è½½: {prev_model_path}")
            model = PPO.load(prev_model_path, env=env, device=device)
            model.tensorboard_log = tensorboard_log
            reset_timesteps = False
    
    return model, reset_timesteps


def main():
    parser = argparse.ArgumentParser(description='SUMOå¤šé˜¶æ®µè®­ç»ƒ')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--stage', type=int, required=True, choices=[1, 2, 3, 4],
                        help='è®­ç»ƒé˜¶æ®µ (1=ç©ºè·¯, 2=çº¢ç»¿ç¯, 3=é¿éšœ, 4=ç»¼åˆ)')
    parser.add_argument('--map', type=str, default='sf_mission',
                        help='åœ°å›¾åç§° (é»˜è®¤: sf_mission)')
    parser.add_argument('--timesteps', type=int, default=None,
                        help='è®­ç»ƒæ­¥æ•° (é»˜è®¤: Stage1=500k, Stage2=800k, Stage3=1M, Stage4=1.5M)')
    parser.add_argument('--n-envs', type=int, default=None,
                        help='å¹¶è¡Œç¯å¢ƒæ•° (é»˜è®¤: 8-16æ ¹æ®ç¡¬ä»¶è‡ªåŠ¨é€‰æ‹©)')
    parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'],
                        help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--from-checkpoint', type=str, default=None,
                        help='ä»æŒ‡å®šcheckpointç»§ç»­è®­ç»ƒ')
    
    # LLMè®­ç»ƒé¡¾é—®å‚æ•°
    parser.add_argument('--llm', '--enable-llm', action='store_true',
                        help='å¯ç”¨LLMè®­ç»ƒé¡¾é—® (Stage 2+)')
    parser.add_argument('--llm-api-key', type=str, default=None,
                        help='Gemini API Key')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--eval-freq', type=int, default=10000,
                        help='è¯„ä¼°é¢‘ç‡ (é»˜è®¤: 10000æ­¥)')
    parser.add_argument('--checkpoint-freq', type=int, default=50000,
                        help='Checkpointä¿å­˜é¢‘ç‡ (é»˜è®¤: 50000æ­¥)')
    parser.add_argument('--gui', action='store_true',
                        help='ä½¿ç”¨SUMO-GUI (ä»…ç”¨äºè°ƒè¯•)')
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤å‚æ•°
    if args.timesteps is None:
        default_timesteps = {1: 500000, 2: 1000000, 3: 1500000, 4: 2000000}
        args.timesteps = default_timesteps[args.stage]
    
    if args.n_envs is None:
        # æ ¹æ®stageè°ƒæ•´å¹¶è¡Œç¯å¢ƒæ•°
        # Stage 1-2: 16ä¸ªç¯å¢ƒï¼ˆæ— èƒŒæ™¯è½¦è¾†ï¼Œè¾ƒå¿«ï¼‰
        # Stage 3-4: 8ä¸ªç¯å¢ƒï¼ˆæœ‰èƒŒæ™¯è½¦è¾†ï¼Œè¾ƒæ…¢ï¼‰
        args.n_envs = 16 if args.stage <= 2 else 8
    
    # æ‰“å°é…ç½®
    print(f"\n{'='*60}")
    print(f"SUMOå¤šé˜¶æ®µè®­ç»ƒ")
    print(f"{'='*60}")
    print(f"é˜¶æ®µ: Stage {args.stage}")
    print(f"åœ°å›¾: {args.map}")
    print(f"è®­ç»ƒæ­¥æ•°: {args.timesteps:,}")
    print(f"å¹¶è¡Œç¯å¢ƒ: {args.n_envs}")
    print(f"è®¾å¤‡: {args.device}")
    print(f"LLMé¡¾é—®: {'å¯ç”¨' if args.llm else 'æœªå¯ç”¨'}")
    print(f"{'='*60}\n")
    
    # æ£€æŸ¥SUMO_HOME
    if 'SUMO_HOME' not in os.environ:
        print("é”™è¯¯: æœªè®¾ç½®ç¯å¢ƒå˜é‡ SUMO_HOME")
        print("   è¯·è®¾ç½®SUMOå®‰è£…è·¯å¾„ï¼Œä¾‹å¦‚:")
        print("   Windows: set SUMO_HOME=C:\\Program Files (x86)\\Eclipse\\Sumo")
        print("   Linux: export SUMO_HOME=/usr/share/sumo")
        sys.exit(1)
    
    # æ£€æŸ¥åœ°å›¾æ–‡ä»¶
    map_file = REPO_DIR / "maps" / f"{args.map}.net.xml"
    if not map_file.exists():
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°åœ°å›¾æ–‡ä»¶ {map_file}")
        print(f"   è¯·å…ˆè¿è¡Œ: python scripts/download_map.py --region {args.map}")
        sys.exit(1)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dirs = [
        REPO_DIR / "outputs" / "models" / f"best_stage{args.stage}",
        REPO_DIR / "outputs" / "logs" / f"stage{args.stage}",
        REPO_DIR / "outputs" / "llm_logs",
    ]
    for d in output_dirs:
        Path(d).mkdir(parents=True, exist_ok=True)
    
    # åˆ›å»ºLLMè®­ç»ƒé¡¾é—®
    llm_advisor = None
    if args.llm:
        llm_advisor = create_llm_advisor(
            stage=args.stage,
            api_key=args.llm_api_key,
            enabled=True
        )
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"ğŸ“¦ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = make_vec_env(args.stage, args.map, args.n_envs)
    
    print(f"ğŸ“¦ åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    eval_env = make_sumo_env(args.stage, args.map, use_gui=args.gui, seed=999)
    eval_env = Monitor(eval_env)
    
    # åˆ›å»ºæˆ–åŠ è½½æ¨¡å‹
    print(f"\nå‡†å¤‡æ¨¡å‹...")
    model, reset_timesteps = create_or_load_model(
        train_env,
        args.stage,
        args.from_checkpoint,
        args.device
    )
    
    # åˆ›å»ºCallbacks
    print(f"\nğŸ”§ è®¾ç½®Callbacks...")
    
    episode_callback = EpisodeStatCallback(
        llm_advisor=llm_advisor,
        log_freq=100,
        verbose=1
    )
    
    eval_callback = EvalCallback(
        eval_env=eval_env,
        eval_freq=args.eval_freq,
        n_eval_episodes=10,
        verbose=1
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=max(args.checkpoint_freq // args.n_envs, 1),  # ä¿®å¤ï¼šéœ€è¦é™¤ä»¥ç¯å¢ƒæ•°
        save_path=str(REPO_DIR / "outputs" / "models" / f"best_stage{args.stage}"),
        name_prefix=f"ppo_stage{args.stage}",
        verbose=1
    )
    
    callbacks = CallbackList([episode_callback, eval_callback, checkpoint_callback])
    
    # æ‰“å°æç¤º
    print(f"\nç›‘æ§è®­ç»ƒ:")
    print(f"   tensorboard --logdir {REPO_DIR}/outputs/logs --reload_interval 5")
    
    if llm_advisor:
        print(f"\nLLMè®­ç»ƒé¡¾é—®:")
        print(f"   - æ¯10000 episodeåˆ†æä¸€æ¬¡")
        print(f"   - æ—¥å¿—ä¿å­˜åˆ°: {REPO_DIR}/outputs/llm_logs/")
    
    print(f"\n{'='*60}")
    print(f"å¼€å§‹è®­ç»ƒ Stage {args.stage}...")
    print(f"{'='*60}\n")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        model.learn(
            total_timesteps=args.timesteps,
            callback=callbacks,
            progress_bar=True,
            reset_num_timesteps=reset_timesteps
        )
    except KeyboardInterrupt:
        print(f"\nè®­ç»ƒè¢«ä¸­æ–­")
    except Exception as e:
        print(f"\nè®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = str(REPO_DIR / "outputs" / "models" / f"best_stage{args.stage}" / "ppo_final.zip")
    model.save(final_model_path)
    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")
    
    # LLMé¡¾é—®æ‘˜è¦
    if llm_advisor:
        print(llm_advisor.get_summary())
    
    # å…³é—­ç¯å¢ƒ
    train_env.close()
    eval_env.close()
    
    # æ£€æŸ¥æˆåŠŸç‡ï¼Œå†³å®šæ˜¯å¦å¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ
    print(f"\n{'='*60}")
    print(f"Stage {args.stage} è®­ç»ƒå®Œæˆ")
    print(f"{'='*60}")
    print(f"è¯·è¿è¡Œè¯„ä¼°è„šæœ¬æ£€æŸ¥æˆåŠŸç‡:")
    print(f"  python scripts/evaluate_sumo.py --stage {args.stage} --n-episodes 100")
    print(f"\nå¦‚æœæˆåŠŸç‡ >= 80%ï¼Œå¯ä»¥è¿›å…¥ä¸‹ä¸€é˜¶æ®µ:")
    if args.stage < 4:
        print(f"  python scripts/train_sumo.py --stage {args.stage + 1}")
    else:
        print(f"  æ­å–œï¼å·²å®Œæˆæ‰€æœ‰è®­ç»ƒé˜¶æ®µï¼")
    print(f"\n")


if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    import multiprocessing
    try:
        multiprocessing.set_start_method('spawn', force=True)
    except:
        pass
    
    main()


